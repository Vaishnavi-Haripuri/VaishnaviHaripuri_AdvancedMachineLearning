# -*- coding: utf-8 -*-
"""#Assignment 1_AML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_SQiOc1SoSRCIvsvilmnfOk70aSACbt

Assignment 1 : Advanced Machine Learnig

by- Vaishnavi Haripuri

student ID- 811285838

Email- vharipur@kent.edu

#Importing data
"""

from tensorflow.keras.datasets import imdb
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

"""#Printing data"""

print(train_data[0])
print(train_labels[0])
[1, 14, 22, 16, ..., 178, 32]
1
print(train_data[0][:10])
# Print the first 10 elements

max([max(sequence) for sequence in train_data])

"""#Preparing data"""

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

"""#Division of training data and test data
#Vectorizing
"""

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

"""#converting labels to floats"""

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

"""#Taking validation set out"""

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

"""#Hypertuning as per conditions

Using 1 hidden layer

using more hidden units

using mse loss function

using tanh activation

#Building network
"""

model = keras.Sequential([
    layers.Dense(64, activation='tanh', input_shape=(10000,)),
    # One hidden layer
    layers.Dense(1, activation='sigmoid')
])

"""#Compilation"""

model.compile(optimizer="rmsprop",
            loss="mse",
            metrics=["accuracy"])

"""#Training model"""

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)

"""#Plotting loss and accuracy scores"""

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
accuracy = history_dict['accuracy']
val_accuracy = history_dict['val_accuracy']

epochs = range(1, len(loss_values) + 1)


plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

predictions = model.predict(x_test)

binary_predictions = (predictions > 0.5).astype(int)

print("Predictions vs Actual:")
for i in range(5):
    print(f"Predicted: {binary_predictions[i][0]}, Actual: {y_test[i]}")

"""#Adding dropout"""

model_dropout = keras.Sequential([
    layers.Dense(64, activation='tanh', input_shape=(10000,)),
    layers.Dropout(0.5),  # 50% dropout
    layers.Dense(1, activation='sigmoid')
])

model_dropout.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])

history_dropout = model_dropout.fit(partial_x_train,
                                     partial_y_train,
                                     epochs=20,
                                     batch_size=512,
                                     validation_data=(x_val, y_val),
                                     verbose=1)

"""#Plotting after adding dropout"""

# Retrieve the history of loss and accuracy from the training process
history_dict = history_dropout.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
accuracy = history_dict['accuracy']
val_accuracy = history_dict['val_accuracy']

epochs = range(1, len(loss_values) + 1)

# Plotting the loss and accuracy

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)  # 1 row, 2 columns, plot 1
plt.plot(epochs, loss_values, 'bo', label='Training loss')  # 'bo' is for blue dot
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')  # 'b' is for solid blue line
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)  # 1 row, 2 columns, plot 2
plt.plot(epochs, accuracy, 'ro', label='Training accuracy')  # 'ro' is for red dot
plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')  # 'r' is for solid red line
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()  # Adjusts subplot spacing to prevent overlap
plt.show()

"""#Evaluate the model on the test set"""

results = model_dropout.evaluate(x_test, y_test)
print("Test Loss, Test Accuracy:", results)

"""# Make predictions on the test set"""

predictions = model_dropout.predict(x_test)

"""# Convert probabilities to binary predictions (0 or 1)

"""

predicted_classes = (predictions > 0.5).astype(int)

"""# Print the first few predictions and their corresponding actual labels

"""

for i in range(5):
    print(f"Predicted: {predicted_classes[i][0]}, Actual: {y_test[i]}")

"""Predicted: 1 means the model predicts a positive review.


Predicted: 0 means the model predicts a negative review.
"""